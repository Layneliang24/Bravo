#!/usr/bin/env python3
"""
Bravo é¡¹ç›®ç˜¦èº«è„šæœ¬
åŠŸèƒ½ï¼š
- æ¸…ç†æ— ç”¨çš„è„šæœ¬æ–‡ä»¶ã€æŠ¥å‘Šæ–‡ä»¶
- åˆ é™¤.keepã€.backupã€.oldç­‰å ä½æ–‡ä»¶
- æ•´ç†é‡å¤æˆ–è¿‡æ—¶çš„é…ç½®æ–‡ä»¶
- ä¿ç•™å¿…è¦çš„æ–‡ä»¶ï¼Œåˆ é™¤å†—ä½™å†…å®¹
"""

import json
import logging
import os
import shutil
from pathlib import Path
from typing import Dict, List, Set

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format="%(levelname)s: %(message)s")
logger = logging.getLogger(__name__)


class ProjectCleaner:
    """é¡¹ç›®æ¸…ç†å™¨"""

    def __init__(self, project_root: str = "."):
        self.project_root = Path(project_root).resolve()
        self.cleaned_files = []
        self.kept_files = []

        # å®šä¹‰æ¸…ç†è§„åˆ™
        self.cleanup_patterns = {
            # å ä½æ–‡ä»¶
            "placeholder_files": [
                "**/*.keep",
                "**/*.backup",
                "**/*.old",
                "**/*.tmp",
                "**/*.temp",
            ],
            # è¿‡æ—¶çš„æµ‹è¯•å’Œå¼€å‘æ–‡ä»¶
            "obsolete_files": [
                # æ ¹ç›®å½•è¿‡æ—¶æ–‡ä»¶
                "test_*.py",
                "fix_*.py",
                "validate_*.py",
                "verify_*.py",
                "check_*.py",
                "ci_validation.py",
                "final_validation_report.*",
                "fix_verification_report.*",
                "gate_optimized.yml",
                "lighthouserc-simple.json",
                "playwright.config.optimized.ts",
                "simulate_ci.sh",
                # é‡å¤çš„Dockeré…ç½®
                "docker-compose.github-actions-simulator.yml",
                "Dockerfile.github-actions-simulator",
                # è¿‡æ—¶çš„æŠ¥å‘Šå’Œæ„å»ºæ–‡ä»¶
                "build.log",
                "bandit-report.json",
                "bandit-report-fixed.json",
                # åç«¯è¿‡æ—¶æ–‡ä»¶
                "backend/health_check_complete.py",
                "backend/health_check_report.txt",
                "backend/run_tests_standalone.py",
                "backend/simple_test_runner.py",
                "backend/test_coverage.py",
                "backend/pytest_simple.ini",
                "backend/celerybeat-schedule",
                "backend/test.db",
                "backend/db.sqlite3",
                # E2Eè¿‡æ—¶æ–‡ä»¶
                "e2e/test-results.xml",
                "e2e/test-results.zip",
                # æ–‡æ¡£ä¸­çš„æµ‹è¯•æ–‡ä»¶
                "docs/test_*.txt",
                "docs/test_*.md",
            ],
            # ç©ºç›®å½•æˆ–åªæœ‰.keepçš„ç›®å½•
            "empty_directories": [
                "tests/data",
                "tests/integration",
                "tests/load",
                "tests/security",
                "tests/system",
                "k8s/base",
                "k8s/helm-bravo",
                "k8s/monitoring",
                "k8s/overlays",
            ],
        }

        # éœ€è¦ä¿ç•™çš„é‡è¦æ–‡ä»¶ï¼ˆå³ä½¿åŒ¹é…æ¸…ç†æ¨¡å¼ï¼‰
        self.protected_files = {
            ".github/workflows/gate.yml.backup",  # é‡è¦å¤‡ä»½
            "docs/02_test_report/",  # æµ‹è¯•æŠ¥å‘Šä¿ç•™
            "tests-golden/",  # é»„é‡‘æµ‹è¯•ä¿ç•™
            "frontend/tests/",  # å‰ç«¯æµ‹è¯•ä¿ç•™
            "backend/tests/",  # åç«¯æµ‹è¯•ä¿ç•™
            "e2e/tests/",  # E2Eæµ‹è¯•ä¿ç•™
        }

    def is_protected(self, file_path: Path) -> bool:
        """æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å—ä¿æŠ¤"""
        str_path = str(file_path.relative_to(self.project_root))
        for protected in self.protected_files:
            if protected in str_path:
                return True
        return False

    def cleanup_placeholder_files(self) -> List[Path]:
        """æ¸…ç†å ä½æ–‡ä»¶"""
        logger.info("ğŸ§¹ æ¸…ç†å ä½æ–‡ä»¶(.keep, .backup, .oldç­‰)...")

        removed_files = []
        for pattern in self.cleanup_patterns["placeholder_files"]:
            for file_path in self.project_root.rglob(pattern):
                if file_path.is_file() and not self.is_protected(file_path):
                    try:
                        file_path.unlink()
                        removed_files.append(file_path)
                        logger.info(
                            f"åˆ é™¤å ä½æ–‡ä»¶: {file_path.relative_to(self.project_root)}"
                        )
                    except Exception as e:
                        logger.error(f"åˆ é™¤å¤±è´¥ {file_path}: {e}")

        return removed_files

    def cleanup_obsolete_files(self) -> List[Path]:
        """æ¸…ç†è¿‡æ—¶æ–‡ä»¶"""
        logger.info("ğŸ—‘ï¸  æ¸…ç†è¿‡æ—¶å’Œå†—ä½™æ–‡ä»¶...")

        removed_files = []
        for pattern in self.cleanup_patterns["obsolete_files"]:
            # å¦‚æœæ˜¯ç›¸å¯¹è·¯å¾„ï¼Œä»é¡¹ç›®æ ¹ç›®å½•æœç´¢
            if "/" in pattern:
                full_pattern = self.project_root / pattern
                if full_pattern.exists():
                    try:
                        if full_pattern.is_file():
                            full_pattern.unlink()
                        elif full_pattern.is_dir():
                            shutil.rmtree(full_pattern)
                        removed_files.append(full_pattern)
                        logger.info(f"åˆ é™¤è¿‡æ—¶æ–‡ä»¶: {pattern}")
                    except Exception as e:
                        logger.error(f"åˆ é™¤å¤±è´¥ {pattern}: {e}")
            else:
                # globæ¨¡å¼æœç´¢
                for file_path in self.project_root.rglob(pattern):
                    if not self.is_protected(file_path):
                        try:
                            if file_path.is_file():
                                file_path.unlink()
                            elif file_path.is_dir():
                                shutil.rmtree(file_path)
                            removed_files.append(file_path)
                            logger.info(
                                f"åˆ é™¤è¿‡æ—¶æ–‡ä»¶: {file_path.relative_to(self.project_root)}"
                            )
                        except Exception as e:
                            logger.error(f"åˆ é™¤å¤±è´¥ {file_path}: {e}")

        return removed_files

    def cleanup_empty_directories(self) -> List[Path]:
        """æ¸…ç†ç©ºç›®å½•"""
        logger.info("ğŸ“ æ¸…ç†ç©ºç›®å½•...")

        removed_dirs = []
        for dir_pattern in self.cleanup_patterns["empty_directories"]:
            dir_path = self.project_root / dir_pattern
            if dir_path.exists() and dir_path.is_dir():
                try:
                    # æ£€æŸ¥ç›®å½•æ˜¯å¦ä¸ºç©ºæˆ–åªæœ‰.keepæ–‡ä»¶
                    contents = list(dir_path.iterdir())
                    if not contents or all(f.name.endswith(".keep") for f in contents):
                        shutil.rmtree(dir_path)
                        removed_dirs.append(dir_path)
                        logger.info(f"åˆ é™¤ç©ºç›®å½•: {dir_pattern}")
                except Exception as e:
                    logger.error(f"åˆ é™¤ç›®å½•å¤±è´¥ {dir_pattern}: {e}")

        return removed_dirs

    def cleanup_duplicate_configs(self) -> List[Path]:
        """æ¸…ç†é‡å¤é…ç½®æ–‡ä»¶"""
        logger.info("âš™ï¸  æ¸…ç†é‡å¤é…ç½®æ–‡ä»¶...")

        removed_files = []

        # æ¸…ç†é‡å¤çš„Dockeré…ç½®
        docker_configs = [
            "docker-compose.prod.yml.keep",
            "docker-compose.test.yml.keep",
            "docker-compose.yml.keep",
            "backend/Dockerfile.keep",
            "backend/Dockerfile.dev.backup",
            "backend/Dockerfile.dev.fixed",
            "frontend/Dockerfile.dev.backup",
            "frontend/Dockerfile.dev.fixed",
        ]

        for config in docker_configs:
            config_path = self.project_root / config
            if config_path.exists():
                try:
                    config_path.unlink()
                    removed_files.append(config_path)
                    logger.info(f"åˆ é™¤é‡å¤é…ç½®: {config}")
                except Exception as e:
                    logger.error(f"åˆ é™¤å¤±è´¥ {config}: {e}")

        # æ¸…ç†é‡å¤çš„å·¥ä½œæµæ–‡ä»¶
        workflow_configs = [
            "e2e/playwright.config.ts.backup",
            "e2e/playwright.config.ts.keep",
            "e2e/package.json.keep",
            "e2e/selenium.config.js.keep",
        ]

        for config in workflow_configs:
            config_path = self.project_root / config
            if config_path.exists():
                try:
                    config_path.unlink()
                    removed_files.append(config_path)
                    logger.info(f"åˆ é™¤é‡å¤å·¥ä½œæµé…ç½®: {config}")
                except Exception as e:
                    logger.error(f"åˆ é™¤å¤±è´¥ {config}: {e}")

        return removed_files

    def cleanup_old_reports(self) -> List[Path]:
        """æ¸…ç†æ—§æŠ¥å‘Šæ–‡ä»¶"""
        logger.info("ğŸ“Š æ¸…ç†æ—§æŠ¥å‘Šæ–‡ä»¶...")

        removed_files = []

        # æ¸…ç†æ ¹ç›®å½•çš„æŠ¥å‘Šæ–‡ä»¶
        report_files = [
            "features.json",  # å·²è¢«æ–°çš„feature-test-map.jsonæ›¿ä»£
        ]

        for report in report_files:
            report_path = self.project_root / report
            if report_path.exists():
                try:
                    report_path.unlink()
                    removed_files.append(report_path)
                    logger.info(f"åˆ é™¤æ—§æŠ¥å‘Š: {report}")
                except Exception as e:
                    logger.error(f"åˆ é™¤å¤±è´¥ {report}: {e}")

        # æ¸…ç†backend/reportsä¸­çš„æ—§æŠ¥å‘Š
        backend_reports_dir = self.project_root / "backend" / "reports"
        if backend_reports_dir.exists():
            for report_file in backend_reports_dir.glob("*.json"):
                # ä¿ç•™æœ€æ–°çš„å‡ ä¸ªæŠ¥å‘Šï¼Œåˆ é™¤è¾ƒè€çš„
                try:
                    # è¿™é‡Œå¯ä»¥æ·»åŠ åŸºäºæ—¶é—´çš„é€»è¾‘
                    pass
                except Exception as e:
                    logger.error(f"å¤„ç†æŠ¥å‘Šæ–‡ä»¶å¤±è´¥ {report_file}: {e}")

        return removed_files

    def optimize_gitignore(self):
        """ä¼˜åŒ–.gitignoreæ–‡ä»¶"""
        logger.info("ğŸ“ ä¼˜åŒ–.gitignore...")

        gitignore_path = self.project_root / ".gitignore"
        if not gitignore_path.exists():
            return

        # è¯»å–ç°æœ‰å†…å®¹
        with open(gitignore_path, "r", encoding="utf-8") as f:
            lines = f.readlines()

        # æ·»åŠ æ–°çš„å¿½ç•¥è§„åˆ™
        new_rules = [
            "\n# æ¸…ç†åæ–°å¢çš„å¿½ç•¥è§„åˆ™\n",
            "*.backup\n",
            "*.old\n",
            "*.tmp\n",
            "*.temp\n",
            "build.log\n",
            "test_*.py\n",
            "fix_*.py\n",
            "validate_*.py\n",
            "verify_*.py\n",
        ]

        # æ£€æŸ¥è§„åˆ™æ˜¯å¦å·²å­˜åœ¨
        existing_content = "".join(lines)
        rules_to_add = []

        for rule in new_rules:
            if rule.strip() and rule.strip() not in existing_content:
                rules_to_add.append(rule)

        if rules_to_add:
            with open(gitignore_path, "a", encoding="utf-8") as f:
                f.writelines(rules_to_add)
            logger.info("å·²æ›´æ–°.gitignoreæ–‡ä»¶")

    def generate_cleanup_report(self, removed_files: List[Path]) -> str:
        """ç”Ÿæˆæ¸…ç†æŠ¥å‘Š"""
        report_path = self.project_root / "reports" / "project_cleanup_report.json"
        report_path.parent.mkdir(exist_ok=True)

        report = {
            "timestamp": "2025-01-13",
            "total_files_removed": len(removed_files),
            "files_by_category": {
                "placeholder_files": [],
                "obsolete_files": [],
                "duplicate_configs": [],
                "empty_directories": [],
                "old_reports": [],
            },
            "space_saved_estimate": "è®¡ç®—ä¸­...",
            "recommendations": [
                "å®šæœŸè¿è¡Œé¡¹ç›®æ¸…ç†è„šæœ¬",
                "é¿å…æäº¤ä¸´æ—¶æ–‡ä»¶å’Œå¤‡ä»½æ–‡ä»¶",
                "ä½¿ç”¨.gitignoreé˜²æ­¢æ— ç”¨æ–‡ä»¶è¿›å…¥ç‰ˆæœ¬æ§åˆ¶",
            ],
        }

        # æŒ‰ç±»åˆ«åˆ†ç±»æ–‡ä»¶
        for file_path in removed_files:
            file_str = str(file_path.relative_to(self.project_root))
            if any(pattern in file_str for pattern in [".keep", ".backup", ".old"]):
                report["files_by_category"]["placeholder_files"].append(file_str)
            elif any(pattern in file_str for pattern in ["test_", "fix_", "validate_"]):
                report["files_by_category"]["obsolete_files"].append(file_str)
            elif "docker-compose" in file_str or "Dockerfile" in file_str:
                report["files_by_category"]["duplicate_configs"].append(file_str)
            elif file_path.is_dir():
                report["files_by_category"]["empty_directories"].append(file_str)
            else:
                report["files_by_category"]["old_reports"].append(file_str)

        try:
            with open(report_path, "w", encoding="utf-8") as f:
                json.dump(report, f, indent=2, ensure_ascii=False)
            logger.info(f"æ¸…ç†æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
            return str(report_path)
        except Exception as e:
            logger.error(f"ä¿å­˜æ¸…ç†æŠ¥å‘Šå¤±è´¥: {e}")
            return ""

    def run_full_cleanup(self) -> Dict:
        """è¿è¡Œå®Œæ•´æ¸…ç†"""
        logger.info("ğŸš€ å¼€å§‹é¡¹ç›®ç˜¦èº«...")

        all_removed_files = []

        # 1. æ¸…ç†å ä½æ–‡ä»¶
        all_removed_files.extend(self.cleanup_placeholder_files())

        # 2. æ¸…ç†è¿‡æ—¶æ–‡ä»¶
        all_removed_files.extend(self.cleanup_obsolete_files())

        # 3. æ¸…ç†ç©ºç›®å½•
        all_removed_files.extend(self.cleanup_empty_directories())

        # 4. æ¸…ç†é‡å¤é…ç½®
        all_removed_files.extend(self.cleanup_duplicate_configs())

        # 5. æ¸…ç†æ—§æŠ¥å‘Š
        all_removed_files.extend(self.cleanup_old_reports())

        # 6. ä¼˜åŒ–.gitignore
        self.optimize_gitignore()

        # 7. ç”Ÿæˆæ¸…ç†æŠ¥å‘Š
        report_path = self.generate_cleanup_report(all_removed_files)

        result = {
            "total_files_removed": len(all_removed_files),
            "removed_files": [
                str(f.relative_to(self.project_root)) for f in all_removed_files
            ],
            "report_path": report_path,
        }

        logger.info(f"âœ… é¡¹ç›®ç˜¦èº«å®Œæˆï¼å…±æ¸…ç† {len(all_removed_files)} ä¸ªæ–‡ä»¶")
        return result


def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description="Bravo é¡¹ç›®ç˜¦èº«å·¥å…·")
    parser.add_argument("--project-root", default=".", help="é¡¹ç›®æ ¹ç›®å½•")
    parser.add_argument("--dry-run", action="store_true", help="é¢„è§ˆæ¨¡å¼ï¼Œä¸å®é™…åˆ é™¤æ–‡ä»¶")

    args = parser.parse_args()

    if args.dry_run:
        logger.info("ğŸ” é¢„è§ˆæ¨¡å¼ - ä¸ä¼šå®é™…åˆ é™¤æ–‡ä»¶")
        # TODO: å®ç°é¢„è§ˆæ¨¡å¼
        return

    cleaner = ProjectCleaner(args.project_root)
    result = cleaner.run_full_cleanup()

    print("\nğŸ“Š æ¸…ç†ç»“æœ:")
    print(f"æ€»è®¡æ¸…ç†æ–‡ä»¶: {result['total_files_removed']}")
    if result["report_path"]:
        print(f"è¯¦ç»†æŠ¥å‘Š: {result['report_path']}")


if __name__ == "__main__":
    main()
