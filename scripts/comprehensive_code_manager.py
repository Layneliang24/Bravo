#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç»¼åˆä»£ç å˜æ›´ç®¡ç†ç³»ç»Ÿ
æ•´åˆä»£ç è¿½è¸ªã€ä¸´æ—¶ä¿®æ”¹æ£€æµ‹å’Œè¿˜åŸéªŒè¯åŠŸèƒ½
"""

import hashlib
import json
import os
import re
import subprocess  # nosec
from dataclasses import asdict, dataclass
from datetime import datetime
from pathlib import Path
from typing import Dict, List


@dataclass
class CodeIssue:
    """ä»£ç é—®é¢˜è®°å½•"""

    file_path: str
    line_number: int
    issue_type: str
    severity: str
    description: str
    code_snippet: str
    context: List[str]
    detected_at: str


class ComprehensiveCodeManager:
    """ç»¼åˆä»£ç ç®¡ç†å™¨"""

    def __init__(self, project_root: str):
        self.project_root = Path(project_root)
        self.reports_dir = self.project_root / "docs" / "02_test_report"
        self.reports_dir.mkdir(parents=True, exist_ok=True)

        # ä¼˜åŒ–çš„æ£€æµ‹è§„åˆ™ - å‡å°‘è¯¯æŠ¥
        self.detection_patterns = {
            "TODO": {
                "pattern": (
                    r"(?i)(?:#|//|/\*|<!--)\s*TODO[:\s]+(.+?)" r"(?:\*/|-->|$)"
                ),
                "severity": "medium",
                "description": "å¾…å®Œæˆä»»åŠ¡",
            },
            "FIXME": {
                "pattern": (
                    r"(?i)(?:#|//|/\*|<!--)\s*FIXME[:\s]+(.+?)" r"(?:\*/|-->|$)"
                ),
                "severity": "high",
                "description": "éœ€è¦ä¿®å¤çš„é—®é¢˜",
            },
            "HACK": {
                "pattern": (
                    r"(?i)(?:#|//|/\*|<!--)\s*HACK[:\s]+(.+?)" r"(?:\*/|-->|$)"
                ),
                "severity": "high",
                "description": "ä¸´æ—¶è§£å†³æ–¹æ¡ˆ",
            },
            "TEMP": {
                "pattern": (
                    r"(?i)(?:#|//|/\*|<!--)\s*TEMP[:\s]+(.+?)" r"(?:\*/|-->|$)"
                ),
                "severity": "medium",
                "description": "ä¸´æ—¶ä»£ç ",
            },
            "DEBUG_PRINT": {
                "pattern": (
                    r"print\s*\([^)]*" r"(?:debug|test|temp|DEBUG|TEST|TEMP)[^)]*\)"
                ),
                "severity": "low",
                "description": "è°ƒè¯•æ‰“å°è¯­å¥",
            },
            "CONSOLE_DEBUG": {
                "pattern": (
                    r"console\.(log|debug)\s*\([^)]*"
                    r"(?:debug|test|temp|DEBUG|TEST|TEMP)[^)]*\)"
                ),
                "severity": "low",
                "description": "è°ƒè¯•æ§åˆ¶å°è¾“å‡º",
            },
            "COMMENTED_FUNCTION": {
                "pattern": (r"^\s*#\s*(def\s+\w+|class\s+\w+|async\s+def\s+\w+)\s*\("),
                "severity": "medium",
                "description": "è¢«æ³¨é‡Šçš„å‡½æ•°æˆ–ç±»å®šä¹‰",
            },
            "COMMENTED_IMPORT": {
                "pattern": r"^\s*#\s*(import\s+\w+|from\s+\w+\s+import)",
                "severity": "low",
                "description": "è¢«æ³¨é‡Šçš„å¯¼å…¥è¯­å¥",
            },
        }

        # æ–‡ä»¶ç±»å‹å’Œæ’é™¤è§„åˆ™
        self.scannable_extensions = {
            ".py",
            ".js",
            ".ts",
            ".vue",
            ".jsx",
            ".tsx",
            ".html",
            ".css",
            ".scss",
            ".sass",
            ".less",
            ".md",
            ".yml",
            ".yaml",
            ".toml",
        }

        self.exclude_patterns = {
            # ç›®å½•
            "node_modules",
            ".git",
            "__pycache__",
            ".pytest_cache",
            "dist",
            "build",
            ".next",
            ".nuxt",
            "coverage",
            ".vscode",
            ".idea",
            ".code_baselines",
            "htmlcov",
            ".mypy_cache",
            "venv",
            "env",
            # æ–‡ä»¶
            "package-lock.json",
            "yarn.lock",
            "poetry.lock",
            ".coverage",
            "coverage.xml",
            # æ¨¡å¼
            r".*\.min\.(js|css)$",
            r".*\.bundle\.(js|css)$",
            r".*\.map$",
        }

    def should_scan_file(self, file_path: Path) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥æ‰«ææ–‡ä»¶"""
        # æ£€æŸ¥æ–‡ä»¶æ‰©å±•å
        if file_path.suffix.lower() not in self.scannable_extensions:
            return False

        # æ£€æŸ¥æ’é™¤æ¨¡å¼
        relative_path = str(file_path.relative_to(self.project_root))

        for pattern in self.exclude_patterns:
            if pattern.startswith("r"):
                # æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼
                regex_pattern = pattern[1:].strip("'\"")
                if re.match(regex_pattern, relative_path):
                    return False
            else:
                # ç®€å•å­—ç¬¦ä¸²åŒ¹é…
                if pattern in relative_path:
                    return False

        return True

    def scan_for_issues(self) -> List[CodeIssue]:
        """æ‰«æä»£ç é—®é¢˜"""
        import time

        start_time = time.time()

        issues = []
        scanned_files = 0
        total_files = 0

        # ç»Ÿè®¡æ€»æ–‡ä»¶æ•°
        for root, dirs, files in os.walk(self.project_root):
            dirs[:] = [
                d
                for d in dirs
                if not any(
                    pattern in d
                    for pattern in self.exclude_patterns
                    if not pattern.startswith("r")
                )
            ]
            for filename in files:
                file_path = Path(root) / filename
                if self.should_scan_file(file_path):
                    total_files += 1

        print(f"[INFO] å¼€å§‹æ‰«æ {total_files} ä¸ªæ–‡ä»¶...")

        for root, dirs, files in os.walk(self.project_root):
            # è¿‡æ»¤ç›®å½•
            dirs[:] = [
                d
                for d in dirs
                if not any(
                    pattern in d
                    for pattern in self.exclude_patterns
                    if not pattern.startswith("r")
                )
            ]

            for filename in files:
                file_path = Path(root) / filename

                if not self.should_scan_file(file_path):
                    continue

                scanned_files += 1
                if scanned_files % 100 == 0:
                    elapsed = time.time() - start_time
                    print(
                        f"[INFO] å·²æ‰«æ {scanned_files}/{total_files} æ–‡ä»¶ "
                        f"({elapsed:.2f}s)"
                    )

                file_issues = self._scan_file_for_issues(file_path)
                issues.extend(file_issues)

        elapsed_time = time.time() - start_time
        print(
            f"[SUCCESS] æ‰«æå®Œæˆ: {scanned_files} æ–‡ä»¶, {len(issues)} é—®é¢˜, "
            f"è€—æ—¶ {elapsed_time:.2f}s"
        )
        print(f"[INFO] å¹³å‡é€Ÿåº¦: {scanned_files / elapsed_time:.1f} æ–‡ä»¶/ç§’")

        return issues

    def _scan_file_for_issues(self, file_path: Path) -> List[CodeIssue]:
        """æ‰«æå•ä¸ªæ–‡ä»¶çš„é—®é¢˜"""
        issues = []

        try:
            with open(file_path, "r", encoding="utf-8", errors="ignore") as f:
                lines = f.readlines()

            for line_num, line in enumerate(lines, 1):
                line_issues = self._check_line_for_issues(
                    file_path, line_num, line, lines
                )
                issues.extend(line_issues)

        except Exception as e:
            print(f"è­¦å‘Š: æ— æ³•è¯»å–æ–‡ä»¶ {file_path}: {e}")

        return issues

    def _check_line_for_issues(
        self, file_path: Path, line_num: int, line: str, all_lines: List[str]
    ) -> List[CodeIssue]:
        """æ£€æŸ¥å•è¡Œä»£ç é—®é¢˜"""
        issues = []

        for issue_type, config in self.detection_patterns.items():
            pattern = config["pattern"]
            match = re.search(pattern, line)

            if match:
                # è·å–ä¸Šä¸‹æ–‡
                context_start = max(0, line_num - 2)
                context_end = min(len(all_lines), line_num + 1)
                context = [
                    f"{i + 1:4d}: {all_lines[i].rstrip()}"
                    for i in range(context_start, context_end)
                ]

                # æå–æè¿°
                description = (
                    match.group(1) if match.groups() else config["description"]
                )
                if not description or not description.strip():
                    description = config["description"]

                issue = CodeIssue(
                    file_path=str(file_path.relative_to(self.project_root)),
                    line_number=line_num,
                    issue_type=issue_type,
                    severity=config["severity"],
                    description=description.strip(),
                    code_snippet=line.strip(),
                    context=context,
                    detected_at=datetime.now().isoformat(),
                )

                issues.append(issue)

        return issues

    def create_baseline(self) -> Dict:
        """åˆ›å»ºé¡¹ç›®åŸºçº¿"""
        print("[INFO] åˆ›å»ºé¡¹ç›®åŸºçº¿...")

        baseline = {
            "timestamp": datetime.now().isoformat(),
            "commit_hash": self._get_git_commit(),
            "features_count": self._count_features(),
            "test_results": self._run_basic_tests(),
            "code_issues": [asdict(issue) for issue in self.scan_for_issues()],
            "file_checksums": self._calculate_key_file_checksums(),
        }

        # ä¿å­˜åŸºçº¿
        baseline_file = self.reports_dir / "project_baseline.json"
        with open(baseline_file, "w", encoding="utf-8") as f:
            json.dump(baseline, f, indent=2, ensure_ascii=False)

        return baseline

    def validate_current_state(self) -> Dict:
        """éªŒè¯å½“å‰çŠ¶æ€"""
        print("[INFO] éªŒè¯å½“å‰é¡¹ç›®çŠ¶æ€...")

        # åŠ è½½åŸºçº¿
        baseline_file = self.reports_dir / "project_baseline.json"
        if not baseline_file.exists():
            return {"status": "no_baseline", "message": "æœªæ‰¾åˆ°åŸºçº¿ï¼Œè¯·å…ˆåˆ›å»ºåŸºçº¿"}

        with open(baseline_file, "r", encoding="utf-8") as f:
            baseline: Dict = json.load(f)

        # è·å–å½“å‰çŠ¶æ€
        current_issues = self.scan_for_issues()
        current_state: Dict = {
            "timestamp": datetime.now().isoformat(),
            "commit_hash": self._get_git_commit(),
            "features_count": self._count_features(),
            "test_results": self._run_basic_tests(),
            "code_issues": [asdict(issue) for issue in current_issues],
            "file_checksums": self._calculate_key_file_checksums(),
        }

        # æ¯”è¾ƒåˆ†æ
        validation = {
            "baseline_timestamp": baseline["timestamp"],
            "current_timestamp": current_state["timestamp"],
            "commit_changed": (baseline["commit_hash"] != current_state["commit_hash"]),
            "features_analysis": self._analyze_features_change(
                baseline["features_count"], current_state["features_count"]
            ),
            "test_analysis": self._analyze_test_change(
                baseline["test_results"], current_state["test_results"]
            ),
            "issues_analysis": self._analyze_issues_change(
                baseline["code_issues"], current_state["code_issues"]
            ),
            "files_analysis": self._analyze_files_change(
                baseline["file_checksums"], current_state["file_checksums"]
            ),
        }

        # è¯„ä¼°æ•´ä½“é£é™©
        validation["overall_risk"] = self._assess_overall_risk(validation)

        return validation

    def _get_git_commit(self) -> str:
        """è·å–Gitæäº¤å“ˆå¸Œ"""
        try:
            result = subprocess.run(  # nosec
                ["git", "rev-parse", "HEAD"],
                cwd=self.project_root,
                capture_output=True,
                text=True,
            )
            return result.stdout.strip() if result.returncode == 0 else "unknown"
        except Exception:
            return "unknown"

    def _count_features(self) -> int:
        """ç»Ÿè®¡åŠŸèƒ½æ•°é‡"""
        features_file = self.project_root / "features.json"
        if features_file.exists():
            try:
                with open(features_file, "r", encoding="utf-8") as f:
                    features = json.load(f)
                return len(features)
            except Exception:
                return 0
        return 0

    def _run_basic_tests(self) -> Dict:
        """è¿è¡ŒåŸºç¡€æµ‹è¯•"""
        try:
            # è¿è¡Œåç«¯ç®€å•æµ‹è¯•
            result = subprocess.run(  # nosec
                ["python", "simple_test_runner.py"],
                cwd=self.project_root / "backend",
                capture_output=True,
                text=True,
                timeout=60,
            )

            if result.returncode == 0:
                # ç®€å•è§£ææµ‹è¯•ç»“æœ
                output_lines = result.stdout.split("\n")
                passed_count = len([line for line in output_lines if "âœ…" in line])
                return {
                    "status": "passed",
                    "passed_tests": passed_count,
                    "pass_rate": 100.0 if passed_count > 0 else 0.0,
                }
            else:
                return {
                    "status": "failed",
                    "passed_tests": 0,
                    "pass_rate": 0.0,
                    "error": result.stderr,
                }
        except Exception as e:
            return {
                "status": "error",
                "passed_tests": 0,
                "pass_rate": 0.0,
                "error": str(e),
            }

    def _calculate_key_file_checksums(self) -> Dict[str, str]:
        """è®¡ç®—å…³é”®æ–‡ä»¶æ ¡éªŒå’Œ"""
        checksums = {}
        key_files = [
            "backend/bravo/settings/base.py",
            "backend/bravo/urls.py",
            "backend/apps/users/models.py",
            "frontend/src/main.ts",
            "frontend/src/App.vue",
            "features.json",
        ]

        for file_path in key_files:
            full_path = self.project_root / file_path
            if full_path.exists():
                with open(full_path, "rb") as f:
                    content = f.read()
                    checksums[file_path] = hashlib.md5(content).hexdigest()  # nosec

        return checksums

    def _analyze_features_change(self, baseline_count: int, current_count: int) -> Dict:
        """åˆ†æåŠŸèƒ½å˜æ›´"""
        change = current_count - baseline_count
        return {
            "baseline_count": baseline_count,
            "current_count": current_count,
            "change": change,
            "status": "increased"
            if change > 0
            else "decreased"
            if change < 0
            else "stable",
        }

    def _analyze_test_change(self, baseline_tests: Dict, current_tests: Dict) -> Dict:
        """åˆ†ææµ‹è¯•å˜æ›´"""
        baseline_rate = baseline_tests.get("pass_rate", 0)
        current_rate = current_tests.get("pass_rate", 0)

        return {
            "baseline_pass_rate": baseline_rate,
            "current_pass_rate": current_rate,
            "change": current_rate - baseline_rate,
            "status": "improved"
            if current_rate > baseline_rate
            else "degraded"
            if current_rate < baseline_rate
            else "stable",
        }

    def _analyze_issues_change(
        self, baseline_issues: List[Dict], current_issues: List[Dict]
    ) -> Dict:
        """åˆ†æä»£ç é—®é¢˜å˜æ›´"""
        baseline_high = len([i for i in baseline_issues if i["severity"] == "high"])
        current_high = len([i for i in current_issues if i["severity"] == "high"])

        baseline_total = len(baseline_issues)
        current_total = len(current_issues)

        return {
            "baseline_total": baseline_total,
            "current_total": current_total,
            "baseline_high_severity": baseline_high,
            "current_high_severity": current_high,
            "total_change": current_total - baseline_total,
            "high_severity_change": current_high - baseline_high,
            "status": "improved"
            if current_high < baseline_high
            else "degraded"
            if current_high > baseline_high
            else "stable",
        }

    def _analyze_files_change(
        self, baseline_checksums: Dict, current_checksums: Dict
    ) -> Dict:
        """åˆ†ææ–‡ä»¶å˜æ›´"""
        changed_files = []
        for file_path, baseline_checksum in baseline_checksums.items():
            current_checksum = current_checksums.get(file_path)
            if current_checksum and current_checksum != baseline_checksum:
                changed_files.append(file_path)

        return {
            "changed_files": changed_files,
            "change_count": len(changed_files),
            "status": "stable" if not changed_files else "changed",
        }

    def _assess_overall_risk(self, validation: Dict) -> str:
        """è¯„ä¼°æ•´ä½“é£é™©"""
        risk_factors = []

        # æµ‹è¯•é€šè¿‡ç‡ä¸‹é™
        test_analysis = validation["test_analysis"]
        if test_analysis["status"] == "degraded":
            risk_factors.append("test_degradation")

        # é«˜ä¸¥é‡æ€§é—®é¢˜å¢åŠ 
        issues_analysis = validation["issues_analysis"]
        if issues_analysis["high_severity_change"] > 5:
            risk_factors.append("high_severity_issues_increase")

        # åŠŸèƒ½æ•°é‡å‡å°‘
        features_analysis = validation["features_analysis"]
        if features_analysis["status"] == "decreased":
            risk_factors.append("features_decrease")

        # å…³é”®æ–‡ä»¶å˜æ›´
        files_analysis = validation["files_analysis"]
        if files_analysis["change_count"] > 3:
            risk_factors.append("many_file_changes")

        # è¯„ä¼°é£é™©ç­‰çº§
        if len(risk_factors) >= 3:
            return "critical"
        elif len(risk_factors) >= 2:
            return "high"
        elif len(risk_factors) >= 1:
            return "medium"
        else:
            return "low"

    def generate_comprehensive_report(self, validation: Dict) -> str:
        """ç”Ÿæˆç»¼åˆæŠ¥å‘Š"""
        lines = []
        lines.append("# ç»¼åˆä»£ç å˜æ›´ç®¡ç†æŠ¥å‘Š")
        lines.append(f"\n**ç”Ÿæˆæ—¶é—´**: {validation['current_timestamp']}")
        lines.append(f"**åŸºçº¿æ—¶é—´**: {validation['baseline_timestamp']}")

        # é£é™©ç­‰çº§
        risk_icons = {"low": "ğŸŸ¢", "medium": "ğŸŸ¡", "high": "ğŸŸ ", "critical": "ğŸ”´"}

        risk = validation["overall_risk"]
        icon = risk_icons.get(risk, "â“")
        lines.append(f"\n{icon} **æ•´ä½“é£é™©ç­‰çº§**: {risk.upper()}")

        # åŠŸèƒ½åˆ†æ
        features = validation["features_analysis"]
        lines.append("\n## ğŸ“‹ åŠŸèƒ½å˜æ›´åˆ†æ")
        lines.append(f"- åŸºçº¿åŠŸèƒ½æ•°: {features['baseline_count']}")
        lines.append(f"- å½“å‰åŠŸèƒ½æ•°: {features['current_count']}")
        lines.append(f"- å˜æ›´æ•°é‡: {features['change']:+d}")

        if features["status"] == "decreased":
            lines.append("- âš ï¸ **è­¦å‘Š**: åŠŸèƒ½æ•°é‡å‡å°‘")
        elif features["status"] == "increased":
            lines.append("- âœ… **è‰¯å¥½**: åŠŸèƒ½æ•°é‡å¢åŠ ")

        # æµ‹è¯•åˆ†æ
        tests = validation["test_analysis"]
        lines.append("\n## ğŸ§ª æµ‹è¯•ç»“æœåˆ†æ")
        lines.append(f"- åŸºçº¿é€šè¿‡ç‡: {tests['baseline_pass_rate']:.1f}%")
        lines.append(f"- å½“å‰é€šè¿‡ç‡: {tests['current_pass_rate']:.1f}%")
        lines.append(f"- å˜åŒ–å¹…åº¦: {tests['change']:+.1f}%")

        if tests["status"] == "degraded":
            lines.append("- ğŸ”´ **è­¦å‘Š**: æµ‹è¯•é€šè¿‡ç‡ä¸‹é™")
        elif tests["status"] == "improved":
            lines.append("- ğŸŸ¢ **è‰¯å¥½**: æµ‹è¯•é€šè¿‡ç‡æå‡")

        # ä»£ç é—®é¢˜åˆ†æ
        issues = validation["issues_analysis"]
        lines.append("\n## ğŸ› ä»£ç é—®é¢˜åˆ†æ")
        lines.append(f"- åŸºçº¿é—®é¢˜æ€»æ•°: {issues['baseline_total']}")
        lines.append(f"- å½“å‰é—®é¢˜æ€»æ•°: {issues['current_total']}")
        lines.append(f"- åŸºçº¿é«˜ä¸¥é‡æ€§: {issues['baseline_high_severity']}")
        lines.append(f"- å½“å‰é«˜ä¸¥é‡æ€§: {issues['current_high_severity']}")
        lines.append(f"- é«˜ä¸¥é‡æ€§å˜åŒ–: {issues['high_severity_change']:+d}")

        if issues["status"] == "degraded":
            lines.append("- ğŸ”´ **è­¦å‘Š**: é«˜ä¸¥é‡æ€§é—®é¢˜å¢åŠ ")
        elif issues["status"] == "improved":
            lines.append("- ğŸŸ¢ **è‰¯å¥½**: é«˜ä¸¥é‡æ€§é—®é¢˜å‡å°‘")

        # æ–‡ä»¶å˜æ›´åˆ†æ
        files = validation["files_analysis"]
        lines.append("\n## ğŸ“ å…³é”®æ–‡ä»¶å˜æ›´")
        lines.append(f"- å˜æ›´æ–‡ä»¶æ•°: {files['change_count']}")

        if files["changed_files"]:
            lines.append("- å˜æ›´çš„æ–‡ä»¶:")
            for file_path in files["changed_files"]:
                lines.append(f"  - {file_path}")

        # å»ºè®®è¡ŒåŠ¨
        lines.append("\n## ğŸ¯ å»ºè®®è¡ŒåŠ¨")

        if risk == "critical":
            lines.append("- ğŸš¨ **ç«‹å³è¡ŒåŠ¨**: å­˜åœ¨ä¸¥é‡é£é™©ï¼Œéœ€è¦ç«‹å³å¤„ç†")
            lines.append("- ğŸ”„ **è€ƒè™‘å›æ»š**: è¯„ä¼°æ˜¯å¦éœ€è¦å›æ»šåˆ°åŸºçº¿çŠ¶æ€")
            lines.append("- ğŸ‘¥ **å›¢é˜Ÿä¼šè®®**: å¬é›†å›¢é˜Ÿè®¨è®ºé—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ")
        elif risk == "high":
            lines.append("- âš ï¸ **ä¼˜å…ˆå¤„ç†**: å­˜åœ¨è¾ƒé«˜é£é™©ï¼Œéœ€è¦ä¼˜å…ˆå…³æ³¨")
            lines.append("- ğŸ” **æ·±å…¥åˆ†æ**: åˆ†æå…·ä½“çš„é—®é¢˜åŸå› ")
            lines.append("- ğŸ“‹ **åˆ¶å®šè®¡åˆ’**: åˆ¶å®šè¯¦ç»†çš„ä¿®å¤è®¡åˆ’")
        elif risk == "medium":
            lines.append("- ğŸ‘€ **æŒç»­å…³æ³¨**: å­˜åœ¨ä¸€å®šé£é™©ï¼Œéœ€è¦å…³æ³¨")
            lines.append("- ğŸ§ª **å¢å¼ºæµ‹è¯•**: è€ƒè™‘å¢åŠ ç›¸å…³æµ‹è¯•ç”¨ä¾‹")
        else:
            lines.append("- ğŸŸ¢ **ä¿æŒç°çŠ¶**: é£é™©è¾ƒä½ï¼Œç»§ç»­æ­£å¸¸å¼€å‘")
            lines.append("- ğŸ”„ **å®šæœŸæ£€æŸ¥**: å»ºè®®å®šæœŸè¿è¡Œæ­¤æ£€æŸ¥")

        return "\n".join(lines)


def main():
    """ä¸»å‡½æ•°"""
    import sys

    project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    manager = ComprehensiveCodeManager(project_root)

    if len(sys.argv) > 1 and sys.argv[1] == "create-baseline":
        baseline = manager.create_baseline()
        print("[SUCCESS] åŸºçº¿å·²åˆ›å»º")
        print(f"[INFO] æäº¤å“ˆå¸Œ: {baseline['commit_hash']}")
        print(f"[INFO] æµ‹è¯•é€šè¿‡ç‡: {baseline['test_results']['pass_rate']:.1f}%")
        print(f"[INFO] ä»£ç é—®é¢˜: {len(baseline['code_issues'])} å¤„")
        return 0

    # éªŒè¯å½“å‰çŠ¶æ€
    validation = manager.validate_current_state()

    if validation.get("status") == "no_baseline":
        print(
            "[ERROR] æœªæ‰¾åˆ°åŸºçº¿ï¼Œè¯·å…ˆè¿è¡Œ: "
            "python comprehensive_code_manager.py create-baseline"
        )
        return 1

    # ç”ŸæˆæŠ¥å‘Š
    report = manager.generate_comprehensive_report(validation)

    # ä¿å­˜æŠ¥å‘Š
    report_file = manager.reports_dir / "comprehensive_code_report.md"
    with open(report_file, "w", encoding="utf-8") as f:
        f.write(report)

    print(f"[SUCCESS] ç»¼åˆæŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")
    print(f"[INFO] æ•´ä½“é£é™©ç­‰çº§: {validation['overall_risk'].upper()}")

    # æ ¹æ®é£é™©ç­‰çº§è¿”å›é€€å‡ºç 
    risk_codes = {"low": 0, "medium": 0, "high": 1, "critical": 2}

    return risk_codes.get(validation["overall_risk"], 1)


if __name__ == "__main__":
    exit(main())
